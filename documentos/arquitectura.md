# üìê Arquitectura de alto nivel - Sistema IA para Escuelas

## üéØ Objetivo

Desarrollar un sistema web que permita a escuelas:

- Cargar documentos y audios.
- Procesar esa informaci√≥n con IA (LLM + Whisper).
- Almacenar y consultar resultados v√≠a RAG.
- Generar planillas y documentos autom√°ticos.
- Escalar seg√∫n demanda con costos controlados.

---

## üèó Arquitectura general

El sistema est√° compuesto por:

### 1Ô∏è‚É£ Frontend Web

- **Framework:** React / Vue / Streamlit
- **Funciones:**

  - Formularios para subir documentos y audios.
  - Configurar prompts personalizados.
  - Visualizar resultados y planillas generadas.
- **Comunicaci√≥n:** HTTP REST hacia el Backend.

---

### 2Ô∏è‚É£ Backend API

- **Framework:** FastAPI (Python) / Flask (o alternativamente Node.js con Express).

- **Funciones:**
  - Orquesta los servicios IA.
  - Decide si usa Ollama local, API externa o GPU remota.
  - Administra l√≥gica de negocio, usuarios y seguridad.
- **Hosting:** VPS en DonWeb con Docker Compose.

---

### 3Ô∏è‚É£ Bases de Datos

- **PostgreSQL (relacional):**
  - Guarda usuarios, documentos, auditor√≠as, configuraciones.
- **ChromaDB (vectorial):**
  - Almacena embeddings para b√∫squedas sem√°nticas (RAG).

Ambas residen inicialmente en el mismo VPS.

---

### 4Ô∏è‚É£ Motores de IA

#### a) Ollama local

- Corre en CPU del VPS.
- Para inferencias econ√≥micas, consultas r√°pidas o sin dependencia externa.

#### b) API externa

- OpenAI (ChatGPT), Gemini (Google), Anthropic (Claude).
- Ideal para resultados premium sin mantener infraestructura.

#### c) GPU remota

- RunPod, LambdaLabs, Vast.ai o AWS EC2/SageMaker con GPU.
- Para grandes lotes de embeddings, transcripciones largas o LLMs complejos.
- Se paga s√≥lo por uso, evitando costos fijos altos.

---

### 5Ô∏è‚É£ Whisper (speech-to-text)

- **Local en VPS:** Para audios cortos.
- **En GPU remota:** Para transcripciones largas, dentro del mismo nodo que el LLM.

---

## ‚öô Orquestaci√≥n y despliegue

- Cada componente encapsulado en Docker.
- **Docker Compose** para manejo local del stack.
- El VPS en DonWeb centraliza el backend, DBs, Ollama y Whisper b√°sicos.
- GPUs y APIs externas se integran v√≠a HTTP, seg√∫n la l√≥gica orquestada por el backend.

---

## üöÄ Escalabilidad

- Al crecer la demanda o tama√±o del vector DB, puede migrarse a:
  - Otro VPS dedicado con m√°s RAM.
  - Servicios gestionados como Pinecone o Weaviate Cloud.
- Si suben las consultas LLM:
  - Balanceo entre varios nodos GPU remotos o upgrade a instancias permanentes.

---

## üìä Diagrama de arquitectura

![Imagen del diagrama](documentos/diagramas/arquitectura.svg)

```plaintext
+-----------------------+
|       Frontend        |
|  (React / Streamlit)  |
+-----------------------+
            |
            v
+----------------------------+
|        Backend API         |
|     (FastAPI / Flask)      |
+----------------------------+
 |          |          |          |
 v          v          v          v
+---+    +------+   +------+   +------+
|SQL|    |Vector|   |Ollama|   |Whisper|
|DB |    |DB    |   |Local |   |Local  |
+---+    +------+   +------+   +------+
 |                        |
 +------------------------+
            |
            v
+----------------------------+
|   Llamadas a LLM externos  |
| (OpenAI, Gemini, GPU node) |
+----------------------------+
````

---

## üß© Stack tecnol√≥gico

| Capa            | Tecnolog√≠as posibles             |
| --------------- | -------------------------------- |
| Frontend        | React, Vue, Streamlit            |
| Backend API     | FastAPI (Python), Flask, Express |
| LLM local       | Ollama (CPU)                     |
| LLM externo     | OpenAI, Gemini, Anthropic        |
| GPU remota      | RunPod, LambdaLabs, Vast.ai, AWS |
| DB relacional   | PostgreSQL                       |
| DB vectorial    | ChromaDB                         |
| Transcripci√≥n   | Whisper CPU / GPU                |
| Contenerizaci√≥n | Docker + Docker Compose          |
| Infraestructura | VPS DonWeb + GPU spot externo    |

---

## ‚úÖ Ventajas

- Costos fijos muy bajos: s√≥lo el VPS.
- Escalable: se usa GPU o API solo cuando se necesita.
- Modular: cada componente puede evolucionar o moverse seg√∫n crecimiento.

---

## üöÄ Pr√≥ximos pasos

- Generar diagramas gr√°ficos (draw\.io / Mermaid).
- Definir m√©tricas y logs clave.
- Planificar escalabilidad (auto-escalado GPU, DBs gestionadas).

---
